{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e20b254-32ca-4624-88ff-77f6be1a5ac7",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d516e8a0-95d7-48f6-835d-ff4286b51cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from fastchat.model import load_model, get_conversation_template, add_model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7a3ad3-ae9d-41ee-9ceb-5c08fe26d8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d95d2e949c461394e65b224c286dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"/workspace/huggingface_hub/vicuna_demo/\"\n",
    "num_of_gpu = 3\n",
    "memory_of_gpu = \"80GiB\"\n",
    "cpu_offlaoding = False\n",
    "load_8bit = False\n",
    "model, tokenizer = load_model(\n",
    "        model_path,\n",
    "        \"cuda\",\n",
    "        num_of_gpu,\n",
    "        memory_of_gpu,\n",
    "        load_8bit,\n",
    "        cpu_offlaoding,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a63bd8-0ee1-4254-9d09-210439297930",
   "metadata": {},
   "source": [
    "## 加载对话模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8c73758-ed89-4c4e-9de9-1b6395659fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = get_conversation_template(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892cf819-5a8b-4a3d-bd5d-9662cbe6ec77",
   "metadata": {},
   "source": [
    "## 开始对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7098f84a-2066-43f1-9f64-2d2cceb95d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: It is a good day today! ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "msg = \"It is a good day today!\"\n",
    "conv.append_message(conv.roles[0], msg)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "769a679a-2fee-4876-9ba6-778f11a151f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: It is a good day today!\n",
      "ASSISTANT: That's great to hear! Is there anything specific you'd like help with?\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer([prompt]).input_ids\n",
    "\n",
    "temperature = 0.7\n",
    "repetition_penalty = 1.0\n",
    "max_new_tokens = 512\n",
    "\n",
    "output_ids = model.generate(\n",
    "    torch.as_tensor(input_ids).cuda(),\n",
    "    do_sample=True,\n",
    "    temperature=temperature,\n",
    "    repetition_penalty=repetition_penalty,\n",
    "    max_new_tokens=max_new_tokens,\n",
    ")\n",
    "\n",
    "output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "outputs = tokenizer.decode(\n",
    "   output_ids, skip_special_tokens=True, spaces_between_special_tokens=False\n",
    ")\n",
    "\n",
    "print(f\"{conv.roles[0]}: {msg}\")\n",
    "print(f\"{conv.roles[1]}: {outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7bdb18c-25c0-403b-b6bc-5f377eedad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: It is a good day today! ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "prompt = conv.get_prompt()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae2929-4f00-451b-bfd6-c0f2f3605b27",
   "metadata": {},
   "source": [
    "## 支持多轮对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7fbeeb53-ae47-4f73-b8d4-617ce89f9dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: It is a good day today! ASSISTANT: That's great to hear! Is there anything specific you'd like help with?</s>\n"
     ]
    }
   ],
   "source": [
    "conv.update_last_message(outputs)\n",
    "prompt = conv.get_prompt()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bb18858-7a0a-41e5-b5f8-cb14b2073116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk(conv, message, temperature = 0.7, repetition_penalty = 1.0, max_new_tokens = 512):\n",
    "    conv.append_message(conv.roles[0], message)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    input_ids = tokenizer([prompt]).input_ids\n",
    "\n",
    "    \n",
    "    output_ids = model.generate(\n",
    "        torch.as_tensor(input_ids).cuda(),\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "\n",
    "    output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "    outputs = tokenizer.decode(\n",
    "        output_ids, skip_special_tokens=True, spaces_between_special_tokens=False\n",
    "    )\n",
    "\n",
    "    print(f\"{conv.roles[0]}: {message}\")\n",
    "    print(f\"{conv.roles[1]}: {outputs}\")\n",
    "    conv.update_last_message(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4682c980-00b9-4b39-948e-10b958955e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Would you like to have a walk in the park?\n",
      "ASSISTANT: Absolutely! I enjoy spending time outdoors when the weather is pleasant. Where would you like to go?\n"
     ]
    }
   ],
   "source": [
    "talk(conv, \"Would you like to have a walk in the park?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ea94a35-b273-41ee-abc6-086c256680ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: It is a good day today! ASSISTANT: That's great to hear! Is there anything specific you'd like help with?</s>USER: Would you like to have a walk in the park? ASSISTANT: Absolutely! I enjoy spending time outdoors when the weather is pleasant. Where would you like to go?</s>\n"
     ]
    }
   ],
   "source": [
    "print(conv.get_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e275dcf1-192e-4bb9-ba03-4113f58ff866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: 你会说中文吗？\n",
      "ASSISTANT: Yes, I know some Chinese. Would you like me to speak Chinese with you?\n"
     ]
    }
   ],
   "source": [
    "talk(conv, \"你会说中文吗？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0811b47-0369-46ac-933f-90778cb9ac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: It is a good day today! ASSISTANT: That's great to hear! Is there anything specific you'd like help with?</s>USER: Would you like to have a walk in the park? ASSISTANT: Absolutely! I enjoy spending time outdoors when the weather is pleasant. Where would you like to go?</s>USER: 你会说中文吗？ ASSISTANT: Yes, I know some Chinese. Would you like me to speak Chinese with you?</s>\n"
     ]
    }
   ],
   "source": [
    "print(conv.get_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4033150-9559-461e-8089-cd4093448d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: \n",
      "Please extract person name from the following sentence and output it in json format:\n",
      "\n",
      "John Doe lives in New York and works at Google.\n",
      "\n",
      "ASSISTANT: Sure! Here's the extracted person name in JSON format: \n",
      "{\n",
      " \"name\": \"John Doe\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"\n",
    "Please extract person name from the following sentence and output it in json format:\n",
    "\n",
    "John Doe lives in New York and works at Google.\n",
    "\"\"\"\n",
    "talk(conv, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eef816b0-8fa7-4e90-9036-47f2197f97df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Please extract person name from the following sentence: \n",
      "\n",
      " John Doe lives in New York and works at Google.\n",
      "ASSISTANT: The extracted person name is \"John Doe\".\n"
     ]
    }
   ],
   "source": [
    "talk(conv, \"Please extract person name from the following sentence: \\n\\n John Doe lives in New York and works at Google.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc7c7e-a9b5-4560-a96d-38410c5cf04f",
   "metadata": {},
   "source": [
    "## 通过Gradio进行封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e577586d-5507-4775-8775-1fe215da5fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gradio/components/textbox.py:259: UserWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "class ConversationWrapper:\n",
    "    def __init__(self):\n",
    "        self.conv = get_conversation_template(model_path)\n",
    "\n",
    "    def talk(self, message):\n",
    "        self.conv.append_message(conv.roles[0], message)\n",
    "        self.conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "        input_ids = tokenizer([prompt]).input_ids\n",
    "    \n",
    "        output_ids = model.generate(\n",
    "            torch.as_tensor(input_ids).cuda(),\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "\n",
    "        output_ids = output_ids[0][len(input_ids[0]) :]\n",
    "        outputs = tokenizer.decode(\n",
    "            output_ids, skip_special_tokens=True, spaces_between_special_tokens=False\n",
    "        )\n",
    "        conv.update_last_message(outputs)\n",
    "        return outputs\n",
    "\n",
    "conv = ConversationWrapper()\n",
    "\n",
    "def predict(input, history=[]):\n",
    "    history.append(input)\n",
    "    response = conv.talk(input)\n",
    "    history.append(response)\n",
    "    responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
    "    return responses, history\n",
    "\n",
    "with gr.Blocks(css=\"#chatbot{height:350px} .overflow-y-auto{height:500px}\") as demo:\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "\n",
    "    txt.submit(predict, [txt, state], [chatbot, state])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1e7f2-3fc6-4714-b66b-1db197a85adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
